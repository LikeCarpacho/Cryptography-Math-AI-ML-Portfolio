{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3720acaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThis code includes modifications from the spinningup repository:\\nhttps://github.com/openai/spinningup\\n\\nProject Title: Vanilla Policy Gradient (VPG) for CartPole-v0\\n\\nFile Name: VPG_cartpole_01_train_save_load.py\\n\\nAuthor: Maciej_M\\n\\nDate: 2023-04-07\\n\\nProject Description:\\nThis project is an implementation of the Vanilla Policy Gradient (VPG) algorithm to solve the CartPole-v0 environment from OpenAI Gym. The VPG algorithm is a foundational reinforcement learning method that uses policy gradients to update an agent's policy directly. The agent is represented by a neural network with a customizable number of hidden layers and hidden units per layer.\\n\\nMain Components:\\n\\n1.mlp function: Creates a multilayer perceptron (MLP) with specified layer sizes and activation functions.\\n2.train function: Trains an agent in the CartPole-v0 environment using the VPG algorithm.\\n3.save_agent function: Saves the trained agent to a file.\\n4.load_agent and load_latest_agent functions: Load a saved agent from a file.\\n5.evaluate_agent function: Evaluates the performance of a trained agent in the CartPole-v0 environment.\\n6.save_plot function: Plots mean average episodes per epoch during training, or in evaluation run.\\n7.Training loop in the if __name__ == '__main__' block: Trains multiple agents with randomly generated hidden layer sizes and numbers of hidden layers, and saves them.\\n8.parse_config, create_logits_net, get_agent_results, and save_best_configs_to_csv functions: These functions are used to test the saved agents, plot their evaluation results, and save the best-performing configurations to a file.\\n\\nResults and best-performing agents can be viewed in results.csv and best_configs.csv.\\n\\nNote: This is a copy of a Jupyter Notebook. There is a bug in the first training iteration of the loop not producing a plot.\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code includes modifications from the spinningup repository:\n",
    "https://github.com/openai/spinningup\n",
    "\n",
    "Project Title: Vanilla Policy Gradient (VPG) for CartPole-v0\n",
    "\n",
    "File Name: VPG_cartpole_01_train_save_load.py\n",
    "\n",
    "Author: Maciej_M\n",
    "\n",
    "Date: 2023-04-07\n",
    "\n",
    "Project Description:\n",
    "This project is an implementation of the Vanilla Policy Gradient (VPG) algorithm to solve the CartPole-v0 environment from OpenAI Gym. The VPG algorithm is a foundational reinforcement learning method that uses policy gradients to update an agent's policy directly. The agent is represented by a neural network with a customizable number of hidden layers and hidden units per layer.\n",
    "\n",
    "Main Components:\n",
    "\n",
    "1.mlp function: Creates a multilayer perceptron (MLP) with specified layer sizes and activation functions.\n",
    "2.train function: Trains an agent in the CartPole-v0 environment using the VPG algorithm.\n",
    "3.save_agent function: Saves the trained agent to a file.\n",
    "4.load_agent and load_latest_agent functions: Load a saved agent from a file.\n",
    "5.evaluate_agent function: Evaluates the performance of a trained agent in the CartPole-v0 environment.\n",
    "6.save_plot function: Plots mean average episodes per epoch during training, or in evaluation run.\n",
    "7.Training loop in the if __name__ == '__main__' block: Trains multiple agents with randomly generated hidden layer sizes and numbers of hidden layers, and saves them.\n",
    "8.parse_config, create_logits_net, get_agent_results, and save_best_configs_to_csv functions: These functions are used to test the saved agents, plot their evaluation results, and save the best-performing configurations to a file.\n",
    "\n",
    "Results and best-performing agents can be viewed in results.csv and best_configs.csv.\n",
    "\n",
    "Note: This is a copy of a Jupyter Notebook. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf0ae74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import imageio\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "from pyvirtualdisplay import Display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0b7258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sizes is dimenion, that is number of vectors in each layer\n",
    "#Then we have a computing function that transforms from each layer to each layer, which is activation\n",
    "#The last operation from one by last layer to the last is an identity.\n",
    "\n",
    "def mlp(sizes, activation=nn.Tanh , output_activation=nn.Identity):\n",
    "    # Build a feedforward neural network.\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48e33c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def save_plot(ep_lengths, filename):\n",
    "    plt.figure()\n",
    "    plt.plot(ep_lengths)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Episode Length')\n",
    "    plt.title('Episode Length over Time')\n",
    "    \n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "233f663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env_name='CartPole-v0', hidden_sizes=[32], lr=1e-2, hidden_layers_number=1,\n",
    "          epochs=50, batch_size=5000, render=False):\n",
    "    \n",
    "    \n",
    "    # make environment, check spaces, get obs / act dims\n",
    "    env = gym.make(env_name)\n",
    "    assert isinstance(env.observation_space, Box),         \"This example only works for envs with continuous state spaces.\"\n",
    "    assert isinstance(env.action_space, Discrete),         \"This example only works for envs with discrete action spaces.\"\n",
    "\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    n_acts = env.action_space.n\n",
    "\n",
    "    # make core of policy network\n",
    "    logits_net = mlp(sizes=[obs_dim] + hidden_sizes * hidden_layers_number + [n_acts])\n",
    "\n",
    "    # make function to compute action distribution\n",
    "    def get_policy(obs):\n",
    "        logits = logits_net(obs)\n",
    "        return Categorical(logits=logits)\n",
    "\n",
    "    # make action selection function (outputs int actions, sampled from policy)\n",
    "    def get_action(obs):\n",
    "        return get_policy(obs).sample().item()\n",
    "\n",
    "    # make loss function whose gradient, for the right data, is policy gradient\n",
    "    def compute_loss(obs, act, weights):\n",
    "        logp = get_policy(obs).log_prob(act)\n",
    "        return -(logp * weights).mean()\n",
    "    # make optimizer\n",
    "    optimizer = torch.optim.Adam(logits_net.parameters(), lr=lr)\n",
    "\n",
    "    # for training policy\n",
    "    def train_one_epoch():\n",
    "        # make some empty lists for logging.\n",
    "        batch_obs = []          # for observations\n",
    "        batch_acts = []         # for actions\n",
    "        batch_weights = []      # for R(tau) weighting in policy gradient\n",
    "        batch_rets = []         # for measuring episode returns\n",
    "        batch_lens = []         # for measuring episode lengths\n",
    "\n",
    "        # reset episode-specific variables\n",
    "        obs = env.reset()       # first obs comes from starting distribution\n",
    "        done = False            # signal from environment that episode is over\n",
    "        ep_rews = []            # list for rewards accrued throughout ep\n",
    "\n",
    "        # render first episode of each epoch\n",
    "        finished_rendering_this_epoch = False\n",
    "\n",
    "        # collect experience by acting in the environment with current policy\n",
    "        while True:\n",
    "\n",
    "            # rendering\n",
    "            if (not finished_rendering_this_epoch) and render:\n",
    "                env.render()\n",
    "\n",
    "            # save obs\n",
    "            batch_obs.append(obs.copy())\n",
    "\n",
    "            # act in the environment\n",
    "            act = get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
    "            obs, rew, done, _ = env.step(act)\n",
    "\n",
    "            # save action, reward\n",
    "            batch_acts.append(act)\n",
    "            ep_rews.append(rew)\n",
    "\n",
    "            if done:\n",
    "                # if episode is over, record info about episode\n",
    "                ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
    "                batch_rets.append(ep_ret)\n",
    "                batch_lens.append(ep_len)\n",
    "\n",
    "                # the weight for each logprob(a|s) is R(tau)\n",
    "                batch_weights += [ep_ret] * ep_len\n",
    "\n",
    "                # reset episode-specific variables\n",
    "                obs, done, ep_rews = env.reset(), False, []\n",
    "\n",
    "                # won't render again this epoch\n",
    "                finished_rendering_this_epoch = True\n",
    "\n",
    "                # end experience loop if we have enough of it\n",
    "                if len(batch_obs) > batch_size:\n",
    "                    break\n",
    "\n",
    "        # take a single policy gradient update step\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32),\n",
    "                                  act=torch.as_tensor(batch_acts, dtype=torch.int32),\n",
    "                                  weights=torch.as_tensor(batch_weights, dtype=torch.float32)\n",
    "                                  )\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        return batch_loss, batch_rets, batch_lens\n",
    "\n",
    "    # training loop\n",
    "    ep_lengths_over_time = [] \n",
    "    for i in range(epochs):\n",
    "        batch_loss, batch_rets, batch_lens = train_one_epoch()\n",
    "        print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n",
    "                (i, batch_loss, np.mean(batch_rets), np.mean(batch_lens)))\n",
    "        ep_lengths_over_time.append(np.mean(batch_lens))\n",
    "    \n",
    "    #save_agent(logits_net,ep_lengths_over_time)\n",
    "    return logits_net, ep_lengths_over_time\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0beddc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_agent(agent, hidden_layers_number, hidden_size, ep_lengths_over_time=None, folder='trained_agents'):\n",
    "    folder_name = f\"VPG_Pytorch_l{hidden_layers_number}_s{hidden_size}\"\n",
    "    save_folder = os.path.join(folder, folder_name)\n",
    "    \n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "\n",
    "    date_str = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    filename = f'agent_{date_str}.pt'\n",
    "\n",
    "    filepath = os.path.join(save_folder, filename)\n",
    "    torch.save(agent.state_dict(), filepath)\n",
    "    print(f\"Agent saved to {filepath}\")\n",
    "\n",
    "    if ep_lengths_over_time is not None:\n",
    "        plot_filepath = filepath.replace('.pt', '_training_plot.png')\n",
    "        save_plot(ep_lengths_over_time, plot_filepath)\n",
    "\n",
    "    return filepath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc4f6b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_agent(logits_net, filepath):\n",
    "    logits_net.load_state_dict(torch.load(filepath))\n",
    "    logits_net.eval()\n",
    "    print(f\"Loaded agent from {filepath}\")\n",
    "    return logits_net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3f7aca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def load_latest_agent(logits_net, folder='trained_agents'):\n",
    "    # Find the latest agent file based on the timestamp in the filename\n",
    "    list_of_files = glob.glob(os.path.join(folder, 'agent_*.pt'))\n",
    "    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "\n",
    "    # Load the agent\n",
    "    logits_net.load_state_dict(torch.load(latest_file))\n",
    "    logits_net.eval()\n",
    "    print(f\"Loaded agent from {latest_file}\")\n",
    "    return logits_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2e717e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, logits_net, num_episodes=50, verbose=True, render=True):\n",
    "    def get_action(obs):\n",
    "        return Categorical(logits=logits_net(torch.as_tensor(obs, dtype=torch.float32).unsqueeze(0))).sample().item()\n",
    "\n",
    "    total_rewards = []\n",
    "    ep_lengths = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        ep_length = 0\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "            action = get_action(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            ep_length += 1\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "        ep_lengths.append(ep_length)\n",
    "        print(f\"Episode {episode + 1}: Total Reward = {total_reward}, Episode Length = {ep_length}\")\n",
    "\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    avg_ep_len = np.mean(ep_lengths)\n",
    "    if verbose:\n",
    "        print(f\"Average Return: {avg_reward:.3f} \\t Average Episode Length: {avg_ep_len:.3f}\")\n",
    "    env.close()\n",
    "    return avg_reward, avg_ep_len,ep_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3533112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    env_name = 'CartPole-v0'\n",
    "    render = False\n",
    "    lr = 1e-2\n",
    "    hidden_sizes = [32]\n",
    "    hidden_layers_number = 1\n",
    "    #hidden_sizes = [32]*hidden_layers_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfc68866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using simplest formulation of policy gradient.\n",
      "\n",
      "Training with 3 hidden layers and hidden size 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/envs/spinningup/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0 \t loss: 18.828 \t return: 21.616 \t ep_len: 21.616\n",
      "epoch:   1 \t loss: 22.225 \t return: 23.947 \t ep_len: 23.947\n",
      "epoch:   2 \t loss: 27.247 \t return: 28.101 \t ep_len: 28.101\n",
      "epoch:   3 \t loss: 26.574 \t return: 29.976 \t ep_len: 29.976\n",
      "epoch:   4 \t loss: 36.243 \t return: 40.772 \t ep_len: 40.772\n",
      "epoch:   5 \t loss: 38.209 \t return: 48.248 \t ep_len: 48.248\n",
      "epoch:   6 \t loss: 41.939 \t return: 51.255 \t ep_len: 51.255\n",
      "epoch:   7 \t loss: 39.079 \t return: 53.737 \t ep_len: 53.737\n",
      "epoch:   8 \t loss: 45.163 \t return: 67.067 \t ep_len: 67.067\n",
      "epoch:   9 \t loss: 42.228 \t return: 66.867 \t ep_len: 66.867\n",
      "epoch:  10 \t loss: 36.571 \t return: 62.837 \t ep_len: 62.837\n",
      "epoch:  11 \t loss: 35.107 \t return: 64.333 \t ep_len: 64.333\n",
      "epoch:  12 \t loss: 32.143 \t return: 62.543 \t ep_len: 62.543\n",
      "epoch:  13 \t loss: 33.283 \t return: 66.117 \t ep_len: 66.117\n",
      "epoch:  14 \t loss: 29.495 \t return: 63.937 \t ep_len: 63.937\n",
      "epoch:  15 \t loss: 30.963 \t return: 67.067 \t ep_len: 67.067\n",
      "epoch:  16 \t loss: 25.647 \t return: 62.099 \t ep_len: 62.099\n",
      "epoch:  17 \t loss: 25.352 \t return: 64.218 \t ep_len: 64.218\n",
      "epoch:  18 \t loss: 27.966 \t return: 71.600 \t ep_len: 71.600\n",
      "epoch:  19 \t loss: 27.470 \t return: 73.551 \t ep_len: 73.551\n",
      "epoch:  20 \t loss: 24.542 \t return: 73.145 \t ep_len: 73.145\n",
      "epoch:  21 \t loss: 22.692 \t return: 67.608 \t ep_len: 67.608\n",
      "epoch:  22 \t loss: 23.878 \t return: 73.662 \t ep_len: 73.662\n",
      "epoch:  23 \t loss: 23.392 \t return: 71.662 \t ep_len: 71.662\n",
      "epoch:  24 \t loss: 26.014 \t return: 78.469 \t ep_len: 78.469\n",
      "epoch:  25 \t loss: 24.192 \t return: 78.484 \t ep_len: 78.484\n",
      "epoch:  26 \t loss: 26.202 \t return: 82.323 \t ep_len: 82.323\n",
      "epoch:  27 \t loss: 23.886 \t return: 74.574 \t ep_len: 74.574\n",
      "epoch:  28 \t loss: 28.143 \t return: 86.224 \t ep_len: 86.224\n",
      "epoch:  29 \t loss: 27.093 \t return: 84.217 \t ep_len: 84.217\n",
      "epoch:  30 \t loss: 26.694 \t return: 87.103 \t ep_len: 87.103\n",
      "epoch:  31 \t loss: 24.321 \t return: 84.933 \t ep_len: 84.933\n",
      "epoch:  32 \t loss: 22.130 \t return: 79.778 \t ep_len: 79.778\n",
      "epoch:  33 \t loss: 20.811 \t return: 78.844 \t ep_len: 78.844\n",
      "epoch:  34 \t loss: 16.111 \t return: 67.080 \t ep_len: 67.080\n",
      "epoch:  35 \t loss: 19.694 \t return: 73.618 \t ep_len: 73.618\n",
      "epoch:  36 \t loss: 17.573 \t return: 73.333 \t ep_len: 73.333\n",
      "epoch:  37 \t loss: 16.154 \t return: 67.560 \t ep_len: 67.560\n",
      "epoch:  38 \t loss: 14.938 \t return: 68.699 \t ep_len: 68.699\n",
      "epoch:  39 \t loss: 15.405 \t return: 68.311 \t ep_len: 68.311\n",
      "epoch:  40 \t loss: 11.558 \t return: 57.529 \t ep_len: 57.529\n",
      "epoch:  41 \t loss: 13.722 \t return: 63.462 \t ep_len: 63.462\n",
      "epoch:  42 \t loss: 11.506 \t return: 63.304 \t ep_len: 63.304\n",
      "epoch:  43 \t loss: 12.848 \t return: 61.354 \t ep_len: 61.354\n",
      "epoch:  44 \t loss: 11.761 \t return: 64.564 \t ep_len: 64.564\n",
      "epoch:  45 \t loss: 16.444 \t return: 78.266 \t ep_len: 78.266\n",
      "epoch:  46 \t loss: 14.479 \t return: 77.652 \t ep_len: 77.652\n",
      "epoch:  47 \t loss: 14.537 \t return: 82.492 \t ep_len: 82.492\n",
      "epoch:  48 \t loss: 12.921 \t return: 86.862 \t ep_len: 86.862\n",
      "epoch:  49 \t loss: 18.527 \t return: 101.780 \t ep_len: 101.780\n",
      "Agent saved to trained_agents/VPG_Pytorch_l3_s26/agent_20230407-174843.pt\n",
      "Training with 3 hidden layers and hidden size 39\n",
      "epoch:   0 \t loss: 17.385 \t return: 20.214 \t ep_len: 20.214\n",
      "epoch:   1 \t loss: 20.004 \t return: 22.850 \t ep_len: 22.850\n",
      "epoch:   2 \t loss: 24.997 \t return: 27.484 \t ep_len: 27.484\n",
      "epoch:   3 \t loss: 30.374 \t return: 34.854 \t ep_len: 34.854\n",
      "epoch:   4 \t loss: 39.513 \t return: 43.983 \t ep_len: 43.983\n",
      "epoch:   5 \t loss: 41.355 \t return: 48.087 \t ep_len: 48.087\n",
      "epoch:   6 \t loss: 32.748 \t return: 42.889 \t ep_len: 42.889\n",
      "epoch:   7 \t loss: 55.127 \t return: 72.913 \t ep_len: 72.913\n",
      "epoch:   8 \t loss: 97.448 \t return: 149.294 \t ep_len: 149.294\n",
      "epoch:   9 \t loss: 95.276 \t return: 156.152 \t ep_len: 156.152\n",
      "epoch:  10 \t loss: 82.663 \t return: 129.897 \t ep_len: 129.897\n",
      "epoch:  11 \t loss: 96.397 \t return: 158.812 \t ep_len: 158.812\n",
      "epoch:  12 \t loss: 83.381 \t return: 137.676 \t ep_len: 137.676\n",
      "epoch:  13 \t loss: 77.421 \t return: 125.325 \t ep_len: 125.325\n",
      "epoch:  14 \t loss: 72.723 \t return: 123.829 \t ep_len: 123.829\n",
      "epoch:  15 \t loss: 79.656 \t return: 136.216 \t ep_len: 136.216\n",
      "epoch:  16 \t loss: 88.582 \t return: 156.212 \t ep_len: 156.212\n",
      "epoch:  17 \t loss: 109.915 \t return: 197.231 \t ep_len: 197.231\n",
      "epoch:  18 \t loss: 110.780 \t return: 196.615 \t ep_len: 196.615\n",
      "epoch:  19 \t loss: 110.778 \t return: 193.923 \t ep_len: 193.923\n",
      "epoch:  20 \t loss: 110.036 \t return: 187.667 \t ep_len: 187.667\n",
      "epoch:  21 \t loss: 111.628 \t return: 194.192 \t ep_len: 194.192\n",
      "epoch:  22 \t loss: 111.447 \t return: 197.577 \t ep_len: 197.577\n",
      "epoch:  23 \t loss: 111.104 \t return: 199.769 \t ep_len: 199.769\n",
      "epoch:  24 \t loss: 107.389 \t return: 194.346 \t ep_len: 194.346\n",
      "epoch:  25 \t loss: 95.934 \t return: 173.379 \t ep_len: 173.379\n",
      "epoch:  26 \t loss: 88.774 \t return: 163.645 \t ep_len: 163.645\n",
      "epoch:  27 \t loss: 83.477 \t return: 156.594 \t ep_len: 156.594\n",
      "epoch:  28 \t loss: 84.501 \t return: 160.969 \t ep_len: 160.969\n",
      "epoch:  29 \t loss: 81.132 \t return: 157.844 \t ep_len: 157.844\n",
      "epoch:  30 \t loss: 84.992 \t return: 174.276 \t ep_len: 174.276\n",
      "epoch:  31 \t loss: 90.657 \t return: 198.538 \t ep_len: 198.538\n",
      "epoch:  32 \t loss: 84.873 \t return: 199.654 \t ep_len: 199.654\n",
      "epoch:  33 \t loss: 81.452 \t return: 199.154 \t ep_len: 199.154\n",
      "epoch:  34 \t loss: 76.124 \t return: 186.296 \t ep_len: 186.296\n",
      "epoch:  35 \t loss: 72.331 \t return: 179.357 \t ep_len: 179.357\n",
      "epoch:  36 \t loss: 69.098 \t return: 176.586 \t ep_len: 176.586\n",
      "epoch:  37 \t loss: 70.396 \t return: 178.929 \t ep_len: 178.929\n",
      "epoch:  38 \t loss: 69.653 \t return: 177.069 \t ep_len: 177.069\n",
      "epoch:  39 \t loss: 62.505 \t return: 160.406 \t ep_len: 160.406\n",
      "epoch:  40 \t loss: 64.243 \t return: 160.406 \t ep_len: 160.406\n",
      "epoch:  41 \t loss: 67.815 \t return: 167.633 \t ep_len: 167.633\n",
      "epoch:  42 \t loss: 77.897 \t return: 190.556 \t ep_len: 190.556\n",
      "epoch:  43 \t loss: 83.786 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  44 \t loss: 83.916 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  45 \t loss: 82.899 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  46 \t loss: 82.427 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  47 \t loss: 81.921 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  48 \t loss: 79.906 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  49 \t loss: 80.691 \t return: 200.000 \t ep_len: 200.000\n",
      "Agent saved to trained_agents/VPG_Pytorch_l3_s39/agent_20230407-175333.pt\n",
      "Training with 3 hidden layers and hidden size 27\n",
      "epoch:   0 \t loss: 19.267 \t return: 21.791 \t ep_len: 21.791\n",
      "epoch:   1 \t loss: 23.278 \t return: 26.824 \t ep_len: 26.824\n",
      "epoch:   2 \t loss: 29.999 \t return: 31.566 \t ep_len: 31.566\n",
      "epoch:   3 \t loss: 33.928 \t return: 38.654 \t ep_len: 38.654\n",
      "epoch:   4 \t loss: 42.165 \t return: 50.780 \t ep_len: 50.780\n",
      "epoch:   5 \t loss: 55.558 \t return: 69.014 \t ep_len: 69.014\n",
      "epoch:   6 \t loss: 63.864 \t return: 86.517 \t ep_len: 86.517\n",
      "epoch:   7 \t loss: 60.986 \t return: 93.815 \t ep_len: 93.815\n",
      "epoch:   8 \t loss: 67.445 \t return: 108.957 \t ep_len: 108.957\n",
      "epoch:   9 \t loss: 71.817 \t return: 125.750 \t ep_len: 125.750\n",
      "epoch:  10 \t loss: 72.272 \t return: 128.385 \t ep_len: 128.385\n",
      "epoch:  11 \t loss: 68.983 \t return: 123.951 \t ep_len: 123.951\n",
      "epoch:  12 \t loss: 79.002 \t return: 145.800 \t ep_len: 145.800\n",
      "epoch:  13 \t loss: 75.047 \t return: 139.361 \t ep_len: 139.361\n",
      "epoch:  14 \t loss: 81.699 \t return: 154.212 \t ep_len: 154.212\n",
      "epoch:  15 \t loss: 79.980 \t return: 152.121 \t ep_len: 152.121\n",
      "epoch:  16 \t loss: 79.250 \t return: 157.531 \t ep_len: 157.531\n",
      "epoch:  17 \t loss: 79.513 \t return: 163.290 \t ep_len: 163.290\n",
      "epoch:  18 \t loss: 82.105 \t return: 167.133 \t ep_len: 167.133\n",
      "epoch:  19 \t loss: 84.709 \t return: 173.655 \t ep_len: 173.655\n",
      "epoch:  20 \t loss: 76.255 \t return: 149.294 \t ep_len: 149.294\n",
      "epoch:  21 \t loss: 71.397 \t return: 137.000 \t ep_len: 137.000\n",
      "epoch:  22 \t loss: 73.985 \t return: 141.361 \t ep_len: 141.361\n",
      "epoch:  23 \t loss: 80.131 \t return: 155.545 \t ep_len: 155.545\n",
      "epoch:  24 \t loss: 78.892 \t return: 149.412 \t ep_len: 149.412\n",
      "epoch:  25 \t loss: 83.888 \t return: 158.938 \t ep_len: 158.938\n",
      "epoch:  26 \t loss: 83.781 \t return: 150.265 \t ep_len: 150.265\n",
      "epoch:  27 \t loss: 71.837 \t return: 125.250 \t ep_len: 125.250\n",
      "epoch:  28 \t loss: 64.256 \t return: 107.426 \t ep_len: 107.426\n",
      "epoch:  29 \t loss: 66.053 \t return: 104.980 \t ep_len: 104.980\n",
      "epoch:  30 \t loss: 70.491 \t return: 128.256 \t ep_len: 128.256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  31 \t loss: 76.297 \t return: 152.500 \t ep_len: 152.500\n",
      "epoch:  32 \t loss: 84.229 \t return: 195.577 \t ep_len: 195.577\n",
      "epoch:  33 \t loss: 79.466 \t return: 197.269 \t ep_len: 197.269\n",
      "epoch:  34 \t loss: 76.030 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  35 \t loss: 71.172 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  36 \t loss: 68.637 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  37 \t loss: 65.365 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  38 \t loss: 62.729 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  39 \t loss: 57.279 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  40 \t loss: 55.274 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  41 \t loss: 57.835 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  42 \t loss: 52.464 \t return: 195.231 \t ep_len: 195.231\n",
      "epoch:  43 \t loss: 48.062 \t return: 198.808 \t ep_len: 198.808\n",
      "epoch:  44 \t loss: 45.662 \t return: 199.077 \t ep_len: 199.077\n",
      "epoch:  45 \t loss: 43.374 \t return: 188.185 \t ep_len: 188.185\n",
      "epoch:  46 \t loss: 42.887 \t return: 187.852 \t ep_len: 187.852\n",
      "epoch:  47 \t loss: 37.342 \t return: 171.700 \t ep_len: 171.700\n",
      "epoch:  48 \t loss: 36.850 \t return: 178.448 \t ep_len: 178.448\n",
      "epoch:  49 \t loss: 32.095 \t return: 166.833 \t ep_len: 166.833\n",
      "Agent saved to trained_agents/VPG_Pytorch_l3_s27/agent_20230407-175832.pt\n",
      "Training with 3 hidden layers and hidden size 34\n",
      "epoch:   0 \t loss: 18.093 \t return: 21.148 \t ep_len: 21.148\n",
      "epoch:   1 \t loss: 23.649 \t return: 26.057 \t ep_len: 26.057\n",
      "epoch:   2 \t loss: 25.494 \t return: 30.840 \t ep_len: 30.840\n",
      "epoch:   3 \t loss: 33.102 \t return: 40.613 \t ep_len: 40.613\n",
      "epoch:   4 \t loss: 37.569 \t return: 50.545 \t ep_len: 50.545\n",
      "epoch:   5 \t loss: 37.779 \t return: 54.446 \t ep_len: 54.446\n",
      "epoch:   6 \t loss: 37.130 \t return: 60.929 \t ep_len: 60.929\n",
      "epoch:   7 \t loss: 41.908 \t return: 72.667 \t ep_len: 72.667\n",
      "epoch:   8 \t loss: 40.292 \t return: 77.800 \t ep_len: 77.800\n",
      "epoch:   9 \t loss: 34.449 \t return: 77.631 \t ep_len: 77.631\n",
      "epoch:  10 \t loss: 30.726 \t return: 75.529 \t ep_len: 75.529\n",
      "epoch:  11 \t loss: 26.755 \t return: 77.969 \t ep_len: 77.969\n",
      "epoch:  12 \t loss: 26.812 \t return: 85.203 \t ep_len: 85.203\n",
      "epoch:  13 \t loss: 23.274 \t return: 85.017 \t ep_len: 85.017\n",
      "epoch:  14 \t loss: 18.242 \t return: 73.559 \t ep_len: 73.559\n",
      "epoch:  15 \t loss: 17.127 \t return: 72.357 \t ep_len: 72.357\n",
      "epoch:  16 \t loss: 9.989 \t return: 55.165 \t ep_len: 55.165\n",
      "epoch:  17 \t loss: 7.911 \t return: 48.000 \t ep_len: 48.000\n",
      "epoch:  18 \t loss: 6.211 \t return: 44.140 \t ep_len: 44.140\n",
      "epoch:  19 \t loss: 5.617 \t return: 39.630 \t ep_len: 39.630\n",
      "epoch:  20 \t loss: 5.649 \t return: 41.000 \t ep_len: 41.000\n",
      "epoch:  21 \t loss: 5.009 \t return: 41.850 \t ep_len: 41.850\n",
      "epoch:  22 \t loss: 5.259 \t return: 42.983 \t ep_len: 42.983\n",
      "epoch:  23 \t loss: 4.041 \t return: 42.627 \t ep_len: 42.627\n",
      "epoch:  24 \t loss: 4.579 \t return: 45.564 \t ep_len: 45.564\n",
      "epoch:  25 \t loss: 4.524 \t return: 50.030 \t ep_len: 50.030\n",
      "epoch:  26 \t loss: 4.244 \t return: 55.088 \t ep_len: 55.088\n",
      "epoch:  27 \t loss: 5.263 \t return: 59.667 \t ep_len: 59.667\n",
      "epoch:  28 \t loss: 5.432 \t return: 64.128 \t ep_len: 64.128\n",
      "epoch:  29 \t loss: 7.866 \t return: 82.403 \t ep_len: 82.403\n",
      "epoch:  30 \t loss: 7.893 \t return: 107.660 \t ep_len: 107.660\n",
      "epoch:  31 \t loss: 9.601 \t return: 109.978 \t ep_len: 109.978\n",
      "epoch:  32 \t loss: 13.110 \t return: 132.154 \t ep_len: 132.154\n",
      "epoch:  33 \t loss: 11.495 \t return: 125.700 \t ep_len: 125.700\n",
      "epoch:  34 \t loss: 10.847 \t return: 119.214 \t ep_len: 119.214\n",
      "epoch:  35 \t loss: 9.688 \t return: 117.349 \t ep_len: 117.349\n",
      "epoch:  36 \t loss: 9.966 \t return: 117.860 \t ep_len: 117.860\n",
      "epoch:  37 \t loss: 8.877 \t return: 133.368 \t ep_len: 133.368\n",
      "epoch:  38 \t loss: 8.584 \t return: 119.395 \t ep_len: 119.395\n",
      "epoch:  39 \t loss: 10.724 \t return: 143.171 \t ep_len: 143.171\n",
      "epoch:  40 \t loss: 10.542 \t return: 153.303 \t ep_len: 153.303\n",
      "epoch:  41 \t loss: 12.243 \t return: 172.067 \t ep_len: 172.067\n",
      "epoch:  42 \t loss: 9.975 \t return: 161.250 \t ep_len: 161.250\n",
      "epoch:  43 \t loss: 10.517 \t return: 161.516 \t ep_len: 161.516\n",
      "epoch:  44 \t loss: 9.786 \t return: 144.771 \t ep_len: 144.771\n",
      "epoch:  45 \t loss: 6.003 \t return: 118.186 \t ep_len: 118.186\n",
      "epoch:  46 \t loss: 6.094 \t return: 103.408 \t ep_len: 103.408\n",
      "epoch:  47 \t loss: 5.869 \t return: 94.868 \t ep_len: 94.868\n",
      "epoch:  48 \t loss: 4.790 \t return: 94.259 \t ep_len: 94.259\n",
      "epoch:  49 \t loss: 4.535 \t return: 91.873 \t ep_len: 91.873\n",
      "Agent saved to trained_agents/VPG_Pytorch_l3_s34/agent_20230407-180320.pt\n",
      "Training with 2 hidden layers and hidden size 22\n",
      "epoch:   0 \t loss: 23.335 \t return: 25.577 \t ep_len: 25.577\n",
      "epoch:   1 \t loss: 28.294 \t return: 30.211 \t ep_len: 30.211\n",
      "epoch:   2 \t loss: 28.404 \t return: 31.400 \t ep_len: 31.400\n",
      "epoch:   3 \t loss: 25.428 \t return: 30.687 \t ep_len: 30.687\n",
      "epoch:   4 \t loss: 30.894 \t return: 38.961 \t ep_len: 38.961\n",
      "epoch:   5 \t loss: 35.946 \t return: 48.699 \t ep_len: 48.699\n",
      "epoch:   6 \t loss: 41.171 \t return: 51.306 \t ep_len: 51.306\n",
      "epoch:   7 \t loss: 36.847 \t return: 47.236 \t ep_len: 47.236\n",
      "epoch:   8 \t loss: 37.193 \t return: 48.000 \t ep_len: 48.000\n",
      "epoch:   9 \t loss: 37.458 \t return: 52.729 \t ep_len: 52.729\n",
      "epoch:  10 \t loss: 37.388 \t return: 53.796 \t ep_len: 53.796\n",
      "epoch:  11 \t loss: 42.579 \t return: 65.143 \t ep_len: 65.143\n",
      "epoch:  12 \t loss: 45.433 \t return: 66.618 \t ep_len: 66.618\n",
      "epoch:  13 \t loss: 45.113 \t return: 66.224 \t ep_len: 66.224\n",
      "epoch:  14 \t loss: 40.013 \t return: 66.592 \t ep_len: 66.592\n",
      "epoch:  15 \t loss: 45.616 \t return: 72.812 \t ep_len: 72.812\n",
      "epoch:  16 \t loss: 43.689 \t return: 73.985 \t ep_len: 73.985\n",
      "epoch:  17 \t loss: 47.767 \t return: 77.615 \t ep_len: 77.615\n",
      "epoch:  18 \t loss: 45.925 \t return: 79.571 \t ep_len: 79.571\n",
      "epoch:  19 \t loss: 47.431 \t return: 82.194 \t ep_len: 82.194\n",
      "epoch:  20 \t loss: 52.978 \t return: 92.741 \t ep_len: 92.741\n",
      "epoch:  21 \t loss: 46.584 \t return: 84.150 \t ep_len: 84.150\n",
      "epoch:  22 \t loss: 52.558 \t return: 96.075 \t ep_len: 96.075\n",
      "epoch:  23 \t loss: 54.349 \t return: 100.220 \t ep_len: 100.220\n",
      "epoch:  24 \t loss: 59.777 \t return: 112.244 \t ep_len: 112.244\n",
      "epoch:  25 \t loss: 61.097 \t return: 114.864 \t ep_len: 114.864\n",
      "epoch:  26 \t loss: 58.092 \t return: 114.818 \t ep_len: 114.818\n",
      "epoch:  27 \t loss: 71.077 \t return: 136.162 \t ep_len: 136.162\n",
      "epoch:  28 \t loss: 70.763 \t return: 139.694 \t ep_len: 139.694\n",
      "epoch:  29 \t loss: 71.876 \t return: 144.200 \t ep_len: 144.200\n",
      "epoch:  30 \t loss: 78.634 \t return: 163.258 \t ep_len: 163.258\n",
      "epoch:  31 \t loss: 78.169 \t return: 157.531 \t ep_len: 157.531\n",
      "epoch:  32 \t loss: 86.189 \t return: 179.357 \t ep_len: 179.357\n",
      "epoch:  33 \t loss: 95.633 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  34 \t loss: 93.135 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  35 \t loss: 93.401 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  36 \t loss: 94.458 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  37 \t loss: 95.706 \t return: 199.154 \t ep_len: 199.154\n",
      "epoch:  38 \t loss: 94.422 \t return: 193.346 \t ep_len: 193.346\n",
      "epoch:  39 \t loss: 97.900 \t return: 197.385 \t ep_len: 197.385\n",
      "epoch:  40 \t loss: 95.733 \t return: 188.481 \t ep_len: 188.481\n",
      "epoch:  41 \t loss: 93.292 \t return: 181.643 \t ep_len: 181.643\n",
      "epoch:  42 \t loss: 90.740 \t return: 172.400 \t ep_len: 172.400\n",
      "epoch:  43 \t loss: 92.968 \t return: 174.379 \t ep_len: 174.379\n",
      "epoch:  44 \t loss: 90.446 \t return: 162.968 \t ep_len: 162.968\n",
      "epoch:  45 \t loss: 87.605 \t return: 159.312 \t ep_len: 159.312\n",
      "epoch:  46 \t loss: 92.491 \t return: 174.621 \t ep_len: 174.621\n",
      "epoch:  47 \t loss: 92.787 \t return: 187.852 \t ep_len: 187.852\n",
      "epoch:  48 \t loss: 82.622 \t return: 163.935 \t ep_len: 163.935\n",
      "epoch:  49 \t loss: 74.994 \t return: 158.406 \t ep_len: 158.406\n",
      "Agent saved to trained_agents/VPG_Pytorch_l2_s22/agent_20230407-180727.pt\n",
      "Training with 1 hidden layers and hidden size 35\n",
      "epoch:   0 \t loss: 15.099 \t return: 18.130 \t ep_len: 18.130\n",
      "epoch:   1 \t loss: 18.306 \t return: 21.004 \t ep_len: 21.004\n",
      "epoch:   2 \t loss: 21.796 \t return: 23.627 \t ep_len: 23.627\n",
      "epoch:   3 \t loss: 24.290 \t return: 27.228 \t ep_len: 27.228\n",
      "epoch:   4 \t loss: 26.797 \t return: 30.131 \t ep_len: 30.131\n",
      "epoch:   5 \t loss: 28.097 \t return: 32.342 \t ep_len: 32.342\n",
      "epoch:   6 \t loss: 29.226 \t return: 33.966 \t ep_len: 33.966\n",
      "epoch:   7 \t loss: 38.948 \t return: 42.420 \t ep_len: 42.420\n",
      "epoch:   8 \t loss: 36.223 \t return: 43.521 \t ep_len: 43.521\n",
      "epoch:   9 \t loss: 38.137 \t return: 47.047 \t ep_len: 47.047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  10 \t loss: 38.471 \t return: 50.020 \t ep_len: 50.020\n",
      "epoch:  11 \t loss: 41.569 \t return: 55.198 \t ep_len: 55.198\n",
      "epoch:  12 \t loss: 42.989 \t return: 56.640 \t ep_len: 56.640\n",
      "epoch:  13 \t loss: 41.270 \t return: 59.988 \t ep_len: 59.988\n",
      "epoch:  14 \t loss: 40.997 \t return: 59.153 \t ep_len: 59.153\n",
      "epoch:  15 \t loss: 38.869 \t return: 56.056 \t ep_len: 56.056\n",
      "epoch:  16 \t loss: 43.661 \t return: 62.675 \t ep_len: 62.675\n",
      "epoch:  17 \t loss: 43.333 \t return: 65.455 \t ep_len: 65.455\n",
      "epoch:  18 \t loss: 41.171 \t return: 62.000 \t ep_len: 62.000\n",
      "epoch:  19 \t loss: 45.493 \t return: 67.040 \t ep_len: 67.040\n",
      "epoch:  20 \t loss: 46.646 \t return: 71.457 \t ep_len: 71.457\n",
      "epoch:  21 \t loss: 41.356 \t return: 62.825 \t ep_len: 62.825\n",
      "epoch:  22 \t loss: 39.656 \t return: 65.192 \t ep_len: 65.192\n",
      "epoch:  23 \t loss: 40.536 \t return: 61.889 \t ep_len: 61.889\n",
      "epoch:  24 \t loss: 45.409 \t return: 66.693 \t ep_len: 66.693\n",
      "epoch:  25 \t loss: 48.294 \t return: 74.776 \t ep_len: 74.776\n",
      "epoch:  26 \t loss: 48.578 \t return: 75.478 \t ep_len: 75.478\n",
      "epoch:  27 \t loss: 53.092 \t return: 80.286 \t ep_len: 80.286\n",
      "epoch:  28 \t loss: 55.028 \t return: 85.051 \t ep_len: 85.051\n",
      "epoch:  29 \t loss: 62.995 \t return: 100.588 \t ep_len: 100.588\n",
      "epoch:  30 \t loss: 69.451 \t return: 112.578 \t ep_len: 112.578\n",
      "epoch:  31 \t loss: 69.965 \t return: 119.286 \t ep_len: 119.286\n",
      "epoch:  32 \t loss: 78.102 \t return: 129.179 \t ep_len: 129.179\n",
      "epoch:  33 \t loss: 83.957 \t return: 140.583 \t ep_len: 140.583\n",
      "epoch:  34 \t loss: 93.526 \t return: 157.344 \t ep_len: 157.344\n",
      "epoch:  35 \t loss: 104.402 \t return: 187.185 \t ep_len: 187.185\n",
      "epoch:  36 \t loss: 101.545 \t return: 185.000 \t ep_len: 185.000\n",
      "epoch:  37 \t loss: 106.613 \t return: 196.077 \t ep_len: 196.077\n",
      "epoch:  38 \t loss: 102.490 \t return: 183.786 \t ep_len: 183.786\n",
      "epoch:  39 \t loss: 100.465 \t return: 179.429 \t ep_len: 179.429\n",
      "epoch:  40 \t loss: 104.654 \t return: 189.593 \t ep_len: 189.593\n",
      "epoch:  41 \t loss: 104.722 \t return: 192.769 \t ep_len: 192.769\n",
      "epoch:  42 \t loss: 104.164 \t return: 189.111 \t ep_len: 189.111\n",
      "epoch:  43 \t loss: 103.168 \t return: 188.222 \t ep_len: 188.222\n",
      "epoch:  44 \t loss: 103.279 \t return: 186.667 \t ep_len: 186.667\n",
      "epoch:  45 \t loss: 105.249 \t return: 198.808 \t ep_len: 198.808\n",
      "epoch:  46 \t loss: 107.286 \t return: 199.423 \t ep_len: 199.423\n",
      "epoch:  47 \t loss: 105.714 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  48 \t loss: 105.254 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  49 \t loss: 104.741 \t return: 198.885 \t ep_len: 198.885\n",
      "Agent saved to trained_agents/VPG_Pytorch_l1_s35/agent_20230407-181100.pt\n",
      "Training with 2 hidden layers and hidden size 32\n",
      "epoch:   0 \t loss: 17.290 \t return: 18.887 \t ep_len: 18.887\n",
      "epoch:   1 \t loss: 19.643 \t return: 21.675 \t ep_len: 21.675\n",
      "epoch:   2 \t loss: 23.546 \t return: 24.842 \t ep_len: 24.842\n",
      "epoch:   3 \t loss: 27.649 \t return: 30.361 \t ep_len: 30.361\n",
      "epoch:   4 \t loss: 29.201 \t return: 35.879 \t ep_len: 35.879\n",
      "epoch:   5 \t loss: 34.785 \t return: 40.752 \t ep_len: 40.752\n",
      "epoch:   6 \t loss: 39.725 \t return: 48.689 \t ep_len: 48.689\n",
      "epoch:   7 \t loss: 38.675 \t return: 52.330 \t ep_len: 52.330\n",
      "epoch:   8 \t loss: 35.580 \t return: 51.173 \t ep_len: 51.173\n",
      "epoch:   9 \t loss: 39.686 \t return: 55.911 \t ep_len: 55.911\n",
      "epoch:  10 \t loss: 41.528 \t return: 61.205 \t ep_len: 61.205\n",
      "epoch:  11 \t loss: 44.674 \t return: 66.408 \t ep_len: 66.408\n",
      "epoch:  12 \t loss: 52.729 \t return: 81.635 \t ep_len: 81.635\n",
      "epoch:  13 \t loss: 48.562 \t return: 82.803 \t ep_len: 82.803\n",
      "epoch:  14 \t loss: 41.626 \t return: 73.824 \t ep_len: 73.824\n",
      "epoch:  15 \t loss: 42.192 \t return: 75.060 \t ep_len: 75.060\n",
      "epoch:  16 \t loss: 46.663 \t return: 86.831 \t ep_len: 86.831\n",
      "epoch:  17 \t loss: 58.999 \t return: 117.186 \t ep_len: 117.186\n",
      "epoch:  18 \t loss: 66.763 \t return: 139.944 \t ep_len: 139.944\n",
      "epoch:  19 \t loss: 55.694 \t return: 113.689 \t ep_len: 113.689\n",
      "epoch:  20 \t loss: 54.713 \t return: 115.841 \t ep_len: 115.841\n",
      "epoch:  21 \t loss: 57.648 \t return: 129.026 \t ep_len: 129.026\n",
      "epoch:  22 \t loss: 61.941 \t return: 141.083 \t ep_len: 141.083\n",
      "epoch:  23 \t loss: 60.280 \t return: 144.286 \t ep_len: 144.286\n",
      "epoch:  24 \t loss: 61.850 \t return: 145.514 \t ep_len: 145.514\n",
      "epoch:  25 \t loss: 58.665 \t return: 148.147 \t ep_len: 148.147\n",
      "epoch:  26 \t loss: 56.611 \t return: 143.657 \t ep_len: 143.657\n",
      "epoch:  27 \t loss: 61.260 \t return: 157.091 \t ep_len: 157.091\n",
      "epoch:  28 \t loss: 61.826 \t return: 162.226 \t ep_len: 162.226\n",
      "epoch:  29 \t loss: 65.214 \t return: 173.966 \t ep_len: 173.966\n",
      "epoch:  30 \t loss: 60.213 \t return: 172.800 \t ep_len: 172.800\n",
      "epoch:  31 \t loss: 62.968 \t return: 181.679 \t ep_len: 181.679\n",
      "epoch:  32 \t loss: 59.360 \t return: 183.143 \t ep_len: 183.143\n",
      "epoch:  33 \t loss: 57.988 \t return: 178.857 \t ep_len: 178.857\n",
      "epoch:  34 \t loss: 56.517 \t return: 176.138 \t ep_len: 176.138\n",
      "epoch:  35 \t loss: 55.758 \t return: 179.138 \t ep_len: 179.138\n",
      "epoch:  36 \t loss: 54.380 \t return: 175.897 \t ep_len: 175.897\n",
      "epoch:  37 \t loss: 53.733 \t return: 168.833 \t ep_len: 168.833\n",
      "epoch:  38 \t loss: 50.895 \t return: 166.581 \t ep_len: 166.581\n",
      "epoch:  39 \t loss: 47.779 \t return: 156.182 \t ep_len: 156.182\n",
      "epoch:  40 \t loss: 46.359 \t return: 153.758 \t ep_len: 153.758\n",
      "epoch:  41 \t loss: 47.861 \t return: 162.645 \t ep_len: 162.645\n",
      "epoch:  42 \t loss: 51.640 \t return: 172.167 \t ep_len: 172.167\n",
      "epoch:  43 \t loss: 52.230 \t return: 182.929 \t ep_len: 182.929\n",
      "epoch:  44 \t loss: 52.673 \t return: 187.407 \t ep_len: 187.407\n",
      "epoch:  45 \t loss: 52.802 \t return: 181.679 \t ep_len: 181.679\n",
      "epoch:  46 \t loss: 48.791 \t return: 189.926 \t ep_len: 189.926\n",
      "epoch:  47 \t loss: 47.779 \t return: 180.321 \t ep_len: 180.321\n",
      "epoch:  48 \t loss: 49.230 \t return: 186.222 \t ep_len: 186.222\n",
      "epoch:  49 \t loss: 49.736 \t return: 190.037 \t ep_len: 190.037\n",
      "Agent saved to trained_agents/VPG_Pytorch_l2_s32/agent_20230407-181510.pt\n",
      "Training with 2 hidden layers and hidden size 36\n",
      "epoch:   0 \t loss: 20.784 \t return: 22.760 \t ep_len: 22.760\n",
      "epoch:   1 \t loss: 27.844 \t return: 28.747 \t ep_len: 28.747\n",
      "epoch:   2 \t loss: 27.993 \t return: 33.126 \t ep_len: 33.126\n",
      "epoch:   3 \t loss: 31.563 \t return: 40.797 \t ep_len: 40.797\n",
      "epoch:   4 \t loss: 37.899 \t return: 50.248 \t ep_len: 50.248\n",
      "epoch:   5 \t loss: 38.542 \t return: 55.473 \t ep_len: 55.473\n",
      "epoch:   6 \t loss: 40.485 \t return: 60.024 \t ep_len: 60.024\n",
      "epoch:   7 \t loss: 38.893 \t return: 62.481 \t ep_len: 62.481\n",
      "epoch:   8 \t loss: 43.681 \t return: 70.859 \t ep_len: 70.859\n",
      "epoch:   9 \t loss: 44.880 \t return: 75.864 \t ep_len: 75.864\n",
      "epoch:  10 \t loss: 51.770 \t return: 85.627 \t ep_len: 85.627\n",
      "epoch:  11 \t loss: 45.816 \t return: 77.923 \t ep_len: 77.923\n",
      "epoch:  12 \t loss: 49.744 \t return: 86.707 \t ep_len: 86.707\n",
      "epoch:  13 \t loss: 59.427 \t return: 109.435 \t ep_len: 109.435\n",
      "epoch:  14 \t loss: 74.043 \t return: 143.139 \t ep_len: 143.139\n",
      "epoch:  15 \t loss: 74.801 \t return: 148.286 \t ep_len: 148.286\n",
      "epoch:  16 \t loss: 72.660 \t return: 152.333 \t ep_len: 152.333\n",
      "epoch:  17 \t loss: 58.679 \t return: 127.350 \t ep_len: 127.350\n",
      "epoch:  18 \t loss: 59.810 \t return: 129.718 \t ep_len: 129.718\n",
      "epoch:  19 \t loss: 58.814 \t return: 135.270 \t ep_len: 135.270\n",
      "epoch:  20 \t loss: 68.653 \t return: 172.552 \t ep_len: 172.552\n",
      "epoch:  21 \t loss: 68.048 \t return: 169.500 \t ep_len: 169.500\n",
      "epoch:  22 \t loss: 47.129 \t return: 117.068 \t ep_len: 117.068\n",
      "epoch:  23 \t loss: 30.009 \t return: 80.698 \t ep_len: 80.698\n",
      "epoch:  24 \t loss: 22.643 \t return: 66.763 \t ep_len: 66.763\n",
      "epoch:  25 \t loss: 18.895 \t return: 58.512 \t ep_len: 58.512\n",
      "epoch:  26 \t loss: 17.565 \t return: 54.097 \t ep_len: 54.097\n",
      "epoch:  27 \t loss: 15.955 \t return: 51.245 \t ep_len: 51.245\n",
      "epoch:  28 \t loss: 14.655 \t return: 50.616 \t ep_len: 50.616\n",
      "epoch:  29 \t loss: 14.170 \t return: 49.304 \t ep_len: 49.304\n",
      "epoch:  30 \t loss: 13.473 \t return: 49.294 \t ep_len: 49.294\n",
      "epoch:  31 \t loss: 12.710 \t return: 50.430 \t ep_len: 50.430\n",
      "epoch:  32 \t loss: 14.055 \t return: 52.674 \t ep_len: 52.674\n",
      "epoch:  33 \t loss: 13.974 \t return: 53.074 \t ep_len: 53.074\n",
      "epoch:  34 \t loss: 13.416 \t return: 55.656 \t ep_len: 55.656\n",
      "epoch:  35 \t loss: 14.918 \t return: 59.776 \t ep_len: 59.776\n",
      "epoch:  36 \t loss: 15.561 \t return: 65.390 \t ep_len: 65.390\n",
      "epoch:  37 \t loss: 16.294 \t return: 67.573 \t ep_len: 67.573\n",
      "epoch:  38 \t loss: 16.186 \t return: 70.153 \t ep_len: 70.153\n",
      "epoch:  39 \t loss: 17.171 \t return: 76.470 \t ep_len: 76.470\n",
      "epoch:  40 \t loss: 17.900 \t return: 75.970 \t ep_len: 75.970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  41 \t loss: 18.974 \t return: 80.871 \t ep_len: 80.871\n",
      "epoch:  42 \t loss: 18.315 \t return: 80.677 \t ep_len: 80.677\n",
      "epoch:  43 \t loss: 20.335 \t return: 88.895 \t ep_len: 88.895\n",
      "epoch:  44 \t loss: 20.269 \t return: 83.917 \t ep_len: 83.917\n",
      "epoch:  45 \t loss: 20.444 \t return: 89.446 \t ep_len: 89.446\n",
      "epoch:  46 \t loss: 22.279 \t return: 93.074 \t ep_len: 93.074\n",
      "epoch:  47 \t loss: 23.492 \t return: 103.429 \t ep_len: 103.429\n",
      "epoch:  48 \t loss: 23.777 \t return: 100.420 \t ep_len: 100.420\n",
      "epoch:  49 \t loss: 27.704 \t return: 112.733 \t ep_len: 112.733\n",
      "Agent saved to trained_agents/VPG_Pytorch_l2_s36/agent_20230407-181927.pt\n",
      "Training with 1 hidden layers and hidden size 32\n",
      "epoch:   0 \t loss: 16.081 \t return: 18.879 \t ep_len: 18.879\n",
      "epoch:   1 \t loss: 17.625 \t return: 20.541 \t ep_len: 20.541\n",
      "epoch:   2 \t loss: 21.421 \t return: 24.539 \t ep_len: 24.539\n",
      "epoch:   3 \t loss: 28.586 \t return: 31.087 \t ep_len: 31.087\n",
      "epoch:   4 \t loss: 29.342 \t return: 32.908 \t ep_len: 32.908\n",
      "epoch:   5 \t loss: 28.118 \t return: 33.272 \t ep_len: 33.272\n",
      "epoch:   6 \t loss: 32.453 \t return: 37.360 \t ep_len: 37.360\n",
      "epoch:   7 \t loss: 35.966 \t return: 42.142 \t ep_len: 42.142\n",
      "epoch:   8 \t loss: 33.491 \t return: 41.694 \t ep_len: 41.694\n",
      "epoch:   9 \t loss: 39.107 \t return: 46.330 \t ep_len: 46.330\n",
      "epoch:  10 \t loss: 41.670 \t return: 50.030 \t ep_len: 50.030\n",
      "epoch:  11 \t loss: 40.184 \t return: 50.350 \t ep_len: 50.350\n",
      "epoch:  12 \t loss: 40.481 \t return: 52.583 \t ep_len: 52.583\n",
      "epoch:  13 \t loss: 40.472 \t return: 55.132 \t ep_len: 55.132\n",
      "epoch:  14 \t loss: 41.589 \t return: 57.908 \t ep_len: 57.908\n",
      "epoch:  15 \t loss: 45.367 \t return: 64.564 \t ep_len: 64.564\n",
      "epoch:  16 \t loss: 39.672 \t return: 59.774 \t ep_len: 59.774\n",
      "epoch:  17 \t loss: 40.796 \t return: 62.049 \t ep_len: 62.049\n",
      "epoch:  18 \t loss: 44.765 \t return: 65.208 \t ep_len: 65.208\n",
      "epoch:  19 \t loss: 47.249 \t return: 72.000 \t ep_len: 72.000\n",
      "epoch:  20 \t loss: 46.226 \t return: 68.589 \t ep_len: 68.589\n",
      "epoch:  21 \t loss: 46.266 \t return: 72.229 \t ep_len: 72.229\n",
      "epoch:  22 \t loss: 48.676 \t return: 75.164 \t ep_len: 75.164\n",
      "epoch:  23 \t loss: 54.673 \t return: 83.650 \t ep_len: 83.650\n",
      "epoch:  24 \t loss: 50.030 \t return: 78.672 \t ep_len: 78.672\n",
      "epoch:  25 \t loss: 48.140 \t return: 79.250 \t ep_len: 79.250\n",
      "epoch:  26 \t loss: 50.951 \t return: 84.617 \t ep_len: 84.617\n",
      "epoch:  27 \t loss: 54.372 \t return: 86.879 \t ep_len: 86.879\n",
      "epoch:  28 \t loss: 58.191 \t return: 91.564 \t ep_len: 91.564\n",
      "epoch:  29 \t loss: 66.198 \t return: 108.021 \t ep_len: 108.021\n",
      "epoch:  30 \t loss: 66.021 \t return: 105.438 \t ep_len: 105.438\n",
      "epoch:  31 \t loss: 80.199 \t return: 132.447 \t ep_len: 132.447\n",
      "epoch:  32 \t loss: 86.049 \t return: 145.971 \t ep_len: 145.971\n",
      "epoch:  33 \t loss: 89.229 \t return: 155.515 \t ep_len: 155.515\n",
      "epoch:  34 \t loss: 92.294 \t return: 163.710 \t ep_len: 163.710\n",
      "epoch:  35 \t loss: 93.195 \t return: 165.065 \t ep_len: 165.065\n",
      "epoch:  36 \t loss: 90.264 \t return: 158.688 \t ep_len: 158.688\n",
      "epoch:  37 \t loss: 96.387 \t return: 173.138 \t ep_len: 173.138\n",
      "epoch:  38 \t loss: 98.696 \t return: 181.357 \t ep_len: 181.357\n",
      "epoch:  39 \t loss: 94.603 \t return: 170.300 \t ep_len: 170.300\n",
      "epoch:  40 \t loss: 98.119 \t return: 175.621 \t ep_len: 175.621\n",
      "epoch:  41 \t loss: 95.618 \t return: 173.233 \t ep_len: 173.233\n",
      "epoch:  42 \t loss: 100.476 \t return: 183.250 \t ep_len: 183.250\n",
      "epoch:  43 \t loss: 103.121 \t return: 194.346 \t ep_len: 194.346\n",
      "epoch:  44 \t loss: 99.101 \t return: 183.286 \t ep_len: 183.286\n",
      "epoch:  45 \t loss: 96.283 \t return: 177.034 \t ep_len: 177.034\n",
      "epoch:  46 \t loss: 97.785 \t return: 181.500 \t ep_len: 181.500\n",
      "epoch:  47 \t loss: 96.880 \t return: 179.393 \t ep_len: 179.393\n",
      "epoch:  48 \t loss: 94.027 \t return: 169.800 \t ep_len: 169.800\n",
      "epoch:  49 \t loss: 92.538 \t return: 170.600 \t ep_len: 170.600\n",
      "Agent saved to trained_agents/VPG_Pytorch_l1_s32/agent_20230407-182248.pt\n",
      "Training with 3 hidden layers and hidden size 22\n",
      "epoch:   0 \t loss: 18.021 \t return: 19.881 \t ep_len: 19.881\n",
      "epoch:   1 \t loss: 19.788 \t return: 22.755 \t ep_len: 22.755\n",
      "epoch:   2 \t loss: 22.547 \t return: 25.333 \t ep_len: 25.333\n",
      "epoch:   3 \t loss: 26.378 \t return: 29.122 \t ep_len: 29.122\n",
      "epoch:   4 \t loss: 28.014 \t return: 31.604 \t ep_len: 31.604\n",
      "epoch:   5 \t loss: 31.757 \t return: 37.485 \t ep_len: 37.485\n",
      "epoch:   6 \t loss: 39.959 \t return: 47.523 \t ep_len: 47.523\n",
      "epoch:   7 \t loss: 45.065 \t return: 56.292 \t ep_len: 56.292\n",
      "epoch:   8 \t loss: 42.836 \t return: 59.583 \t ep_len: 59.583\n",
      "epoch:   9 \t loss: 46.649 \t return: 67.838 \t ep_len: 67.838\n",
      "epoch:  10 \t loss: 55.626 \t return: 81.500 \t ep_len: 81.500\n",
      "epoch:  11 \t loss: 56.416 \t return: 90.982 \t ep_len: 90.982\n",
      "epoch:  12 \t loss: 56.916 \t return: 95.396 \t ep_len: 95.396\n",
      "epoch:  13 \t loss: 52.330 \t return: 91.655 \t ep_len: 91.655\n",
      "epoch:  14 \t loss: 56.777 \t return: 107.319 \t ep_len: 107.319\n",
      "epoch:  15 \t loss: 66.424 \t return: 135.378 \t ep_len: 135.378\n",
      "epoch:  16 \t loss: 71.777 \t return: 153.182 \t ep_len: 153.182\n",
      "epoch:  17 \t loss: 67.122 \t return: 143.714 \t ep_len: 143.714\n",
      "epoch:  18 \t loss: 71.530 \t return: 152.394 \t ep_len: 152.394\n",
      "epoch:  19 \t loss: 66.786 \t return: 143.657 \t ep_len: 143.657\n",
      "epoch:  20 \t loss: 65.729 \t return: 142.278 \t ep_len: 142.278\n",
      "epoch:  21 \t loss: 62.595 \t return: 134.447 \t ep_len: 134.447\n",
      "epoch:  22 \t loss: 65.487 \t return: 134.816 \t ep_len: 134.816\n",
      "epoch:  23 \t loss: 60.776 \t return: 131.487 \t ep_len: 131.487\n",
      "epoch:  24 \t loss: 59.352 \t return: 130.462 \t ep_len: 130.462\n",
      "epoch:  25 \t loss: 60.609 \t return: 133.447 \t ep_len: 133.447\n",
      "epoch:  26 \t loss: 58.212 \t return: 133.868 \t ep_len: 133.868\n",
      "epoch:  27 \t loss: 60.846 \t return: 141.278 \t ep_len: 141.278\n",
      "epoch:  28 \t loss: 65.708 \t return: 151.735 \t ep_len: 151.735\n",
      "epoch:  29 \t loss: 68.909 \t return: 161.355 \t ep_len: 161.355\n",
      "epoch:  30 \t loss: 59.523 \t return: 146.314 \t ep_len: 146.314\n",
      "epoch:  31 \t loss: 54.665 \t return: 129.769 \t ep_len: 129.769\n",
      "epoch:  32 \t loss: 47.298 \t return: 118.814 \t ep_len: 118.814\n",
      "epoch:  33 \t loss: 38.448 \t return: 93.296 \t ep_len: 93.296\n",
      "epoch:  34 \t loss: 31.547 \t return: 83.000 \t ep_len: 83.000\n",
      "epoch:  35 \t loss: 30.703 \t return: 85.847 \t ep_len: 85.847\n",
      "epoch:  36 \t loss: 38.210 \t return: 104.458 \t ep_len: 104.458\n",
      "epoch:  37 \t loss: 42.808 \t return: 125.244 \t ep_len: 125.244\n",
      "epoch:  38 \t loss: 42.656 \t return: 126.075 \t ep_len: 126.075\n",
      "epoch:  39 \t loss: 42.799 \t return: 125.450 \t ep_len: 125.450\n",
      "epoch:  40 \t loss: 45.829 \t return: 145.714 \t ep_len: 145.714\n",
      "epoch:  41 \t loss: 50.122 \t return: 162.613 \t ep_len: 162.613\n",
      "epoch:  42 \t loss: 52.089 \t return: 176.069 \t ep_len: 176.069\n",
      "epoch:  43 \t loss: 48.726 \t return: 169.400 \t ep_len: 169.400\n",
      "epoch:  44 \t loss: 40.771 \t return: 143.029 \t ep_len: 143.029\n",
      "epoch:  45 \t loss: 40.432 \t return: 148.294 \t ep_len: 148.294\n",
      "epoch:  46 \t loss: 40.651 \t return: 139.811 \t ep_len: 139.811\n",
      "epoch:  47 \t loss: 39.085 \t return: 134.579 \t ep_len: 134.579\n",
      "epoch:  48 \t loss: 43.397 \t return: 149.500 \t ep_len: 149.500\n",
      "epoch:  49 \t loss: 38.510 \t return: 137.784 \t ep_len: 137.784\n",
      "Agent saved to trained_agents/VPG_Pytorch_l3_s22/agent_20230407-182735.pt\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = Args()\n",
    "    print('\\nUsing simplest formulation of policy gradient.\\n')\n",
    "\n",
    "    # Randomly test 10 hidden layer sizes around 32 and number from 1 to 3\n",
    "    for _ in range(10):\n",
    "        hidden_size = np.random.randint(32 - 10, 32 + 10)  # Random size around 32\n",
    "        hidden_layers_number = np.random.randint(1, 4)  # Random number of layers from 1 to 3\n",
    "        args.hidden_sizes = [hidden_size]\n",
    "        args.hidden_layers_number = hidden_layers_number\n",
    "\n",
    "        print(f\"Training with {hidden_layers_number} hidden layers and hidden size {hidden_size}\")\n",
    "\n",
    "        # Train the agent\n",
    "        logits_net, ep_lengths_over_time = train(env_name=args.env_name, hidden_sizes=args.hidden_sizes,\n",
    "                           lr=args.lr, hidden_layers_number=args.hidden_layers_number,\n",
    "                           render=args.render)\n",
    "\n",
    "        # Save the agent and the training plot\n",
    "        save_agent(logits_net, args.hidden_layers_number, hidden_size, ep_lengths_over_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60070ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_config(config_str):\n",
    "    match = re.search(r'l(\\d+)_s(\\d+)', config_str)\n",
    "    num_hidden_layers = int(match.group(1))\n",
    "    hidden_size = int(match.group(2))\n",
    "    return hidden_size, num_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58df5b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logits_net(obs_dim, n_acts, hidden_size, num_hidden_layers):\n",
    "    layer_sizes = [obs_dim] + [hidden_size] * num_hidden_layers + [n_acts]\n",
    "    return mlp(layer_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d04a18fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_agent_results(folder='trained_agents'):\n",
    "    results = []\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.pt'):\n",
    "                env_name = 'CartPole-v0'\n",
    "                env = gym.make(env_name)\n",
    "                obs_dim = env.observation_space.shape[0]\n",
    "                n_acts = env.action_space.n\n",
    "                filepath = os.path.join(root, file)\n",
    "                config = os.path.basename(root)\n",
    "                hidden_size, num_hidden_layers = parse_config(config)\n",
    "                logits_net = create_logits_net(obs_dim, n_acts, hidden_size, num_hidden_layers)\n",
    "                agent = load_agent(logits_net, filepath)\n",
    "                \n",
    "                if not os.path.exists('evaluation_plots'):\n",
    "                    os.makedirs('evaluation_plots')\n",
    "                avg_return, avg_ep_len, ep_lengths = evaluate_agent(env, agent, num_episodes=5, verbose=False, render=False)\n",
    "                date_str = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "                plot_filename = os.path.basename(file).replace('.pt', f'_training_plot_{date_str}.png')\n",
    "                plot_filepath = os.path.join('evaluation_plots', plot_filename)\n",
    "                save_plot(ep_lengths, plot_filepath)\n",
    "                results.append({'config': config, 'avg_return': avg_return, 'avg_ep_len': avg_ep_len})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75faa279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_configs_to_csv(results, filename='results.csv', best_configs_filename='best_configs.txt'):\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "    max_avg_return = df['avg_return'].max()\n",
    "    best_configs = df[df['avg_return'] == max_avg_return]['config'].values\n",
    "\n",
    "    with open(best_configs_filename, 'w') as f:\n",
    "        for config in best_configs:\n",
    "            f.write(config + '\\n')\n",
    "\n",
    "    return best_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "733fdfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/envs/spinningup/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded agent from trained_agents/VPG_Pytorch_l1_s32/agent_20230407-182248.pt\n",
      "Episode 1: Total Reward = 167.0, Episode Length = 167\n",
      "Episode 2: Total Reward = 166.0, Episode Length = 166\n",
      "Episode 3: Total Reward = 125.0, Episode Length = 125\n",
      "Episode 4: Total Reward = 128.0, Episode Length = 128\n",
      "Episode 5: Total Reward = 200.0, Episode Length = 200\n",
      "Loaded agent from trained_agents/VPG_Pytorch_l3_s22/agent_20230407-182735.pt\n",
      "Episode 1: Total Reward = 148.0, Episode Length = 148\n",
      "Episode 2: Total Reward = 109.0, Episode Length = 109\n",
      "Episode 3: Total Reward = 195.0, Episode Length = 195\n",
      "Episode 4: Total Reward = 157.0, Episode Length = 157\n",
      "Episode 5: Total Reward = 83.0, Episode Length = 83\n",
      "Loaded agent from trained_agents/VPG_Pytorch_l3_s27/agent_20230407-175832.pt\n",
      "Episode 1: Total Reward = 194.0, Episode Length = 194\n",
      "Episode 2: Total Reward = 196.0, Episode Length = 196\n",
      "Episode 3: Total Reward = 84.0, Episode Length = 84\n",
      "Episode 4: Total Reward = 134.0, Episode Length = 134\n",
      "Episode 5: Total Reward = 188.0, Episode Length = 188\n",
      "Loaded agent from trained_agents/VPG_Pytorch_l3_s39/agent_20230407-175333.pt\n",
      "Episode 1: Total Reward = 200.0, Episode Length = 200\n",
      "Episode 2: Total Reward = 200.0, Episode Length = 200\n",
      "Episode 3: Total Reward = 200.0, Episode Length = 200\n",
      "Episode 4: Total Reward = 200.0, Episode Length = 200\n",
      "Episode 5: Total Reward = 200.0, Episode Length = 200\n",
      "Loaded agent from trained_agents/VPG_Pytorch_l2_s36/agent_20230407-181927.pt\n",
      "Episode 1: Total Reward = 186.0, Episode Length = 186\n",
      "Episode 2: Total Reward = 117.0, Episode Length = 117\n",
      "Episode 3: Total Reward = 124.0, Episode Length = 124\n",
      "Episode 4: Total Reward = 98.0, Episode Length = 98\n",
      "Episode 5: Total Reward = 97.0, Episode Length = 97\n",
      "Loaded agent from trained_agents/VPG_Pytorch_l1_s35/agent_20230407-181100.pt\n",
      "Episode 1: Total Reward = 200.0, Episode Length = 200\n",
      "Episode 2: Total Reward = 200.0, Episode Length = 200\n",
      "Episode 3: Total Reward = 200.0, Episode Length = 200\n",
      "Episode 4: Total Reward = 200.0, Episode Length = 200\n",
      "Episode 5: Total Reward = 200.0, Episode Length = 200\n",
      "Loaded agent from trained_agents/VPG_Pytorch_l2_s32/agent_20230407-181510.pt\n",
      "Episode 1: Total Reward = 200.0, Episode Length = 200\n",
      "Episode 2: Total Reward = 200.0, Episode Length = 200\n",
      "Episode 3: Total Reward = 193.0, Episode Length = 193\n",
      "Episode 4: Total Reward = 167.0, Episode Length = 167\n",
      "Episode 5: Total Reward = 156.0, Episode Length = 156\n",
      "Loaded agent from trained_agents/VPG_Pytorch_l3_s26/agent_20230407-174843.pt\n",
      "Episode 1: Total Reward = 74.0, Episode Length = 74\n",
      "Episode 2: Total Reward = 80.0, Episode Length = 80\n",
      "Episode 3: Total Reward = 84.0, Episode Length = 84\n",
      "Episode 4: Total Reward = 142.0, Episode Length = 142\n",
      "Episode 5: Total Reward = 78.0, Episode Length = 78\n",
      "Loaded agent from trained_agents/VPG_Pytorch_l2_s22/agent_20230407-180727.pt\n",
      "Episode 1: Total Reward = 147.0, Episode Length = 147\n",
      "Episode 2: Total Reward = 37.0, Episode Length = 37\n",
      "Episode 3: Total Reward = 21.0, Episode Length = 21\n",
      "Episode 4: Total Reward = 129.0, Episode Length = 129\n",
      "Episode 5: Total Reward = 104.0, Episode Length = 104\n",
      "Loaded agent from trained_agents/VPG_Pytorch_l3_s34/agent_20230407-180320.pt\n",
      "Episode 1: Total Reward = 72.0, Episode Length = 72\n",
      "Episode 2: Total Reward = 82.0, Episode Length = 82\n",
      "Episode 3: Total Reward = 61.0, Episode Length = 61\n",
      "Episode 4: Total Reward = 64.0, Episode Length = 64\n",
      "Episode 5: Total Reward = 104.0, Episode Length = 104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/envs/spinningup/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded agent from trained_agents/VPG_Pytorch_l3_s39/agent_20230407-175333.pt\n",
      "Evaluating agent with config: VPG_Pytorch_l3_s39\n",
      "Episode 1: Total Reward = 200.0, Episode Length = 200\n",
      "Episode 2: Total Reward = 200.0, Episode Length = 200\n",
      "Episode 3: Total Reward = 200.0, Episode Length = 200\n",
      "Episode 4: Total Reward = 200.0, Episode Length = 200\n",
      "Episode 5: Total Reward = 200.0, Episode Length = 200\n",
      "Average Return: 200.000 \t Average Episode Length: 200.000\n",
      "Loaded agent from trained_agents/VPG_Pytorch_l1_s35/agent_20230407-181100.pt\n",
      "Evaluating agent with config: VPG_Pytorch_l1_s35\n",
      "Episode 1: Total Reward = 200.0, Episode Length = 200\n",
      "Episode 2: Total Reward = 200.0, Episode Length = 200\n",
      "Episode 3: Total Reward = 200.0, Episode Length = 200\n",
      "Episode 4: Total Reward = 200.0, Episode Length = 200\n",
      "Episode 5: Total Reward = 200.0, Episode Length = 200\n",
      "Average Return: 200.000 \t Average Episode Length: 200.000\n"
     ]
    }
   ],
   "source": [
    "results = get_agent_results()\n",
    "best_configs = save_best_configs_to_csv(results)\n",
    "env_name = 'CartPole-v0'\n",
    "env = gym.make(env_name)\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "n_acts = env.action_space.n\n",
    "for best_config in best_configs:\n",
    "    # Load the best agent\n",
    "    best_agent_folder = os.path.join('trained_agents', best_config)\n",
    "    list_of_files = glob.glob(os.path.join(best_agent_folder, 'agent_*.pt'))\n",
    "    best_agent_path = max(list_of_files, key=os.path.getctime)\n",
    "\n",
    "    # Get hidden_size and num_hidden_layers from best_config\n",
    "    hidden_size, num_hidden_layers = parse_config(best_config)\n",
    "\n",
    "    # Create logits_net for best agent\n",
    "    best_logits_net = create_logits_net(obs_dim, n_acts, hidden_size, num_hidden_layers)\n",
    "\n",
    "    # Load agent\n",
    "    best_agent = load_agent(best_logits_net, best_agent_path)\n",
    "\n",
    "    # Evaluate the best agent\n",
    "    print(f\"Evaluating agent with config: {best_config}\")\n",
    "    evaluate_agent(env, best_agent, num_episodes=5, verbose=True, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec93b85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
